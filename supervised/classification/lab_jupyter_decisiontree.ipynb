{"cells":[{"cell_type":"markdown","id":"4f94ecda-c728-4251-b26a-c3032bd5aa43","metadata":{},"outputs":[],"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n","    </a>\n","</p>\n"]},{"cell_type":"markdown","id":"17681893-65cd-4898-8f55-dfa5352c8fb3","metadata":{},"outputs":[],"source":["# **Decision Tree**\n"]},{"cell_type":"markdown","id":"714949da-d00e-4dfe-a2d8-b9492aeb1133","metadata":{},"outputs":[],"source":["Estimated time needed: **30** minutes\n"]},{"cell_type":"markdown","id":"086b7b37-8011-476c-876c-c9372ffc81cc","metadata":{},"outputs":[],"source":["In this lab, you will learn and practice the decision tree model. Decision tree model outputs a set of rules, and each rule is a If-else chain from root to a leaf node. Decision tree mimics human reasoning process which makes it very intuitive to human users and makes it a high-interpretable model.\n"]},{"cell_type":"markdown","id":"af65ae35-d06d-47f3-bf82-7dabd8bb71bf","metadata":{},"outputs":[],"source":["We will be using a tumor sample dataset, which contains lab test results about tumor samples. The objective is to classify whether a tumor is malicious (cancer) or benign. As such, it is a typical binary classification task.\n"]},{"cell_type":"markdown","id":"d24e069a-f854-4623-9f08-f5a2541c031c","metadata":{},"outputs":[],"source":["## Objectives\n"]},{"cell_type":"markdown","id":"5040f329-e6a2-4fe9-8b24-47771160f9fb","metadata":{},"outputs":[],"source":["After completing this lab you will be able to:\n"]},{"cell_type":"markdown","id":"00e1a3c2-afe6-4bb2-a56e-33f8072c58cd","metadata":{},"outputs":[],"source":["* Train decision tree models with customized hyperparameters\n","* Evaluate decision tree models on classification tasks\n","* Visualize decision tree models by plotting the tree\n","* Tune the hyperparameters to find the optimized one for a specific task\n"]},{"cell_type":"markdown","id":"f70c7796-d581-40a1-a385-416c33e7bded","metadata":{},"outputs":[],"source":["----\n"]},{"cell_type":"markdown","id":"bd97633d-5689-4b59-8b9e-da84d42c0989","metadata":{},"outputs":[],"source":["## Lab Environment Setup\n"]},{"cell_type":"markdown","id":"bbfeab00-0dc6-4751-bcf7-55ac0c72495a","metadata":{},"outputs":[],"source":["Instal and import required packages\n"]},{"cell_type":"code","id":"805bac5e-1e1a-43b5-8efa-3e0963e3cbd0","metadata":{},"outputs":[],"source":["# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n# !mamba install -qy pandas==1.3.3 numpy==1.21.2 ipywidgets==7.4.2 scipy==7.4.2 tqdm==4.62.3 matplotlib==3.5.0 seaborn==0.9.0\n# Note: If your environment doesn't support \"!mamba install\", use \"!pip install\" "]},{"cell_type":"code","id":"d27c3eb9-aee3-4ac8-8420-15a4537655d2","metadata":{},"outputs":[],"source":["!pip install --upgrade scikit-learn"]},{"cell_type":"code","id":"1036136d-9347-4570-9ff4-927de8d6cbb5","metadata":{},"outputs":[],"source":["import pandas as pd\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n# Evaluation metrics related methods\nfrom sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_recall_fscore_support, precision_score, recall_score\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline"]},{"cell_type":"code","id":"3f705d94-c6db-4182-861b-7173c5e552c9","metadata":{},"outputs":[],"source":["rs = 123"]},{"cell_type":"markdown","id":"812016f9-9fd0-46ae-b9ae-f91c9fa58ffd","metadata":{},"outputs":[],"source":["## Load and explore the tumor dataset\n"]},{"cell_type":"markdown","id":"bb10accb-50ff-41cd-be09-6db913c1d9f4","metadata":{},"outputs":[],"source":["First, let's load the tumor dataset as a Pandas dataframe\n"]},{"cell_type":"code","id":"a748911a-0d39-4b69-8cd6-acc3b0b313a7","metadata":{},"outputs":[],"source":["dataset_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML241EN-SkillsNetwork/labs/datasets/tumor.csv\"\ntumor_df = pd.read_csv(dataset_url)"]},{"cell_type":"markdown","id":"bd3cf2d0-5265-4445-aa6e-9cad0a0aa4aa","metadata":{},"outputs":[],"source":["And check its dataframe head\n"]},{"cell_type":"code","id":"ca3d7650-d5ef-4c31-a24c-c75f0703cfd1","metadata":{},"outputs":[],"source":["tumor_df.head()"]},{"cell_type":"markdown","id":"170d54a6-856d-410c-9e74-09e2113e3423","metadata":{},"outputs":[],"source":["Each observation in this dataset contains lab tests results about a tumor sample, such as clump or shapes. Based on these lab test results or features, we want to build a classification model to predict if this tumor sample is malicious (cancer) and benign. The target variable `y` is specified in the `Class` column.\n"]},{"cell_type":"markdown","id":"064a7d74-5a97-41c5-943b-bcd8455c3d55","metadata":{},"outputs":[],"source":["Then, let's split the dataframe into train and testing data\n"]},{"cell_type":"code","id":"9c46fa2e-8e03-48f7-814f-c06c56fd2a78","metadata":{},"outputs":[],"source":["# Get the input features\nX = tumor_df.iloc[:, :-1]\n# Get the target variable\ny = tumor_df.iloc[:, -1:]"]},{"cell_type":"code","id":"8ac83a3b-80fe-489f-bc50-328c3651eb71","metadata":{},"outputs":[],"source":["# Split the training and testing data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state = rs)"]},{"cell_type":"markdown","id":"74abe307-8694-4d81-a2bf-7195981fc860","metadata":{},"outputs":[],"source":["### Train a default decision tree\n"]},{"cell_type":"markdown","id":"4545aae7-bd57-401d-a3ad-8973329d3649","metadata":{},"outputs":[],"source":["Training a decision classifier is very straightforward with `sklearn`, we first need to define a `DecisionTreeClassifier` object. In the first step, we will use all the default arguments.\n"]},{"cell_type":"code","id":"3e075965-5d03-4319-a9e3-41fd983fb919","metadata":{},"outputs":[],"source":["# Train a decision tree with all default arguments\nmodel = DecisionTreeClassifier(random_state=rs)"]},{"cell_type":"markdown","id":"a8123893-fbf9-4e6a-8cb1-b13e58fae626","metadata":{},"outputs":[],"source":["Then we can train the decision tree model with training and testing data\n"]},{"cell_type":"code","id":"f1d611e6-c5f4-4401-b232-024fec90fdb6","metadata":{},"outputs":[],"source":["model.fit(X_train, y_train.values.ravel())"]},{"cell_type":"markdown","id":"e3a1189a-ebfc-4c60-a5b1-d945142b1985","metadata":{},"outputs":[],"source":["And make predictions on the test data\n"]},{"cell_type":"code","id":"f5815748-170a-4dd6-be2e-302da39e7ef4","metadata":{},"outputs":[],"source":["preds = model.predict(X_test)"]},{"cell_type":"markdown","id":"9e2137fb-45ee-4d76-ac36-b765bc62fa0f","metadata":{},"outputs":[],"source":["Here we also provided a utility method to evaluate the trained decision tree model and output some standard evaluation metrics.\n"]},{"cell_type":"code","id":"3685c398-e042-40d1-93fc-daea1fd1e4dd","metadata":{},"outputs":[],"source":["def evaluate_metrics(yt, yp):\n    results_pos = {}\n    results_pos['accuracy'] = accuracy_score(yt, yp)\n    precision, recall, f_beta, _ = precision_recall_fscore_support(yt, yp, average='binary')\n    results_pos['recall'] = recall\n    results_pos['precision'] = precision\n    results_pos['f1score'] = f_beta\n    return results_pos"]},{"cell_type":"code","id":"7bc1edc1-2a62-4150-a925-3d6baa8888ed","metadata":{},"outputs":[],"source":["evaluate_metrics(y_test, preds)"]},{"cell_type":"markdown","id":"faef01d5-55f1-4ad7-9ea0-b2abf3e701bb","metadata":{},"outputs":[],"source":["Now we can see that the trained decision model has very good classification results on the testing data, with a very high F1 score around 0.94. Next, let's try to visualize and interpret the trained decision tree model.\n"]},{"cell_type":"markdown","id":"56cf15a9-503c-44d0-824d-9a78d9e6e715","metadata":{},"outputs":[],"source":["## Visualize the trained decision tree\n"]},{"cell_type":"markdown","id":"5fe0278b-f4a1-4fc3-85d5-0ec97d5efb9f","metadata":{},"outputs":[],"source":["We will be using the `tree.plot_tree()` method provided by `sklearn` to quickly plot any decision tree model.\n"]},{"cell_type":"code","id":"5ee9e0e2-059a-4413-808d-ac3e51852cc5","metadata":{},"outputs":[],"source":["def plot_decision_tree(model, feature_names):\n    plt.subplots(figsize=(25, 20)) \n    tree.plot_tree(model, \n                       feature_names=feature_names,  \n                       filled=True)\n    plt.show()"]},{"cell_type":"code","id":"b032b04b-a9e9-4b22-8bcb-553c06bb15c6","metadata":{},"outputs":[],"source":["feature_names = X.columns.values"]},{"cell_type":"code","id":"76fa052c-d202-4372-8fc1-72f4233e6904","metadata":{},"outputs":[],"source":["plot_decision_tree(model, feature_names)"]},{"cell_type":"markdown","id":"b4cfef9f-5b4d-4c13-a468-8a492284fe07","metadata":{},"outputs":[],"source":["And you should see a relatively complex decision tree model being plotted. First, you may notice the decision tree is color-labeled, orange node means a majority of samples in the node belong to `Class 0` and blue node means a majority of samples in the node belong to `Class 1`, and white node means it has an equal amount of `Class 0` and `Class 1` samples.\n"]},{"cell_type":"markdown","id":"c9cf5c98-cd02-4d32-899b-314ed1e2b104","metadata":{},"outputs":[],"source":["Because the tree is very big, so the rules and split threshold on each node are very difficult to see. In addition, big decision trees may easily bring large variance and cause overfitting. Next, let's try to build simplified decision trees, and hopefully the simplified decision trees may generate even better results.\n"]},{"cell_type":"markdown","id":"c331cfcf-f144-4d2b-b6d3-73a557110098","metadata":{},"outputs":[],"source":["## Cutomize the decision tree model\n"]},{"cell_type":"markdown","id":"3a5932ff-b6aa-46bd-a2c6-83a1ccb97e5a","metadata":{},"outputs":[],"source":["The `DecisionTreeClassifier` has many arguments (model hyperparameters) that can be customized and eventually tune the generated decision tree classifiers. Among these arguments, there are three commonly tuned arguments as follows:\n","- criterion: `gini` or `entropy`, which specifies which criteria to be used when splitting a tree node\n","- max_depth: a numeric value to specify the max depth of the tree. Larger tree depth normally means larger model complexity\n","- min_samples_leaf: The minimal number of samples in leaf nodes. Larger samples in leaf nodes will tend to generate simpler trees\n"]},{"cell_type":"markdown","id":"bbe41f6d-3180-4130-bebf-6cb3369b5e87","metadata":{},"outputs":[],"source":["Let's first try the following hyperparameter values:\n","- criterion = 'entropy'\n","- max_depth = 10\n","- min_samples_leaf=3\n"]},{"cell_type":"code","id":"b20d0612-fcbf-425e-8de5-7c490401d81d","metadata":{},"outputs":[],"source":["# criterion = 'entropy'\n# max_depth = 10\n# min_samples_leaf=3\ncustom_model = DecisionTreeClassifier(criterion='entropy', max_depth=10, min_samples_leaf=3, random_state=rs)"]},{"cell_type":"markdown","id":"054d4c24-98ea-460b-8d3c-9f6c30abf528","metadata":{},"outputs":[],"source":["And let's train and evaluate the customized model\n"]},{"cell_type":"code","id":"0a84468d-0a11-4923-8e75-10fa3e2e1879","metadata":{},"outputs":[],"source":["custom_model.fit(X_train, y_train.values.ravel())\npreds = custom_model.predict(X_test)\nevaluate_metrics(y_test, preds)"]},{"cell_type":"markdown","id":"ff9bd6cb-35ef-454e-9945-884d0168c049","metadata":{},"outputs":[],"source":["Its F1 score has increased to 0.946 now, which seems better than the previous default decision tree model.\n"]},{"cell_type":"markdown","id":"4143e694-7376-4d72-9713-a8e58ebf9bd7","metadata":{},"outputs":[],"source":["Then, let's visualize the custom model using plot_decision_tree() utility method we created in the previous step:\n"]},{"cell_type":"code","id":"a93ae2a8-795f-4455-a964-31947dde0848","metadata":{},"outputs":[],"source":["# Plot the decision tree\nplot_decision_tree(custom_model, feature_names)"]},{"cell_type":"markdown","id":"4e948632-3e77-4810-9feb-24d85efd9def","metadata":{},"outputs":[],"source":["As you can see the tuned decision tree above is much simpler than the default decision tree model. We can see from each node, it's split feature and threshold, and entropy difference before and after a split.\n"]},{"cell_type":"markdown","id":"d00247f5-7180-4f09-9a22-feac22b606ba","metadata":{},"outputs":[],"source":["## Coding exercise: build and visualize a decision tree with criterion='gini', max_depth = 15, and min_samples_leaf=5\n"]},{"cell_type":"code","id":"ae5c5197-2a73-4c07-958f-d704245447a6","metadata":{},"outputs":[],"source":["# Type your code here\n"]},{"cell_type":"markdown","id":"2a477619-ee19-4105-aeec-f965316e5618","metadata":{},"outputs":[],"source":["<details><summary>Click here for a sample solution</summary>\n","\n","```python\n","# Train a customized decision tree\n","custom_model = DecisionTreeClassifier(criterion='gini', max_depth=15, min_samples_leaf=5, random_state=rs)\n","custom_model.fit(X_train, y_train.values.ravel())\n","preds = custom_model.predict(X_test)\n","evaluate_metrics(y_test, preds)\n","# Plot the decision tree\n","plot_decision_tree(custom_model, feature_names)\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"41e9e4f4-c2c3-40d8-aa9b-d340fcd58a07","metadata":{},"outputs":[],"source":["## Tune hyperparameters\n"]},{"cell_type":"markdown","id":"66d8e70f-1e42-4977-80e7-5af231bde8b1","metadata":{},"outputs":[],"source":["Lastly, let's try to find the optimized hyperparameters, which can produce the highest F1 score, via GridSearch cross-validation.\n"]},{"cell_type":"markdown","id":"9bfcee66-cbdd-4f5e-92d8-beb54b7306e9","metadata":{},"outputs":[],"source":["We define a `params_grid` dict object to contain the parameter candidates:\n"]},{"cell_type":"code","id":"4f732e6a-3a32-4a8f-a382-0cb19a933001","metadata":{},"outputs":[],"source":["params_grid = {\n    'criterion': ['gini', 'entropy'],\n    'max_depth': [5, 10, 15, 20],\n    'min_samples_leaf': [1, 2, 5]\n}"]},{"cell_type":"markdown","id":"8860c944-2d79-4c9c-9985-f159f6a9d062","metadata":{},"outputs":[],"source":["Then we create a default decision tree classifier to be tuned:\n"]},{"cell_type":"code","id":"ac215bc7-7f0f-4937-8013-b0f7484c255c","metadata":{},"outputs":[],"source":["model = DecisionTreeClassifier(random_state=rs)"]},{"cell_type":"markdown","id":"655ad5e1-00d2-4f04-b314-ac2c9726d073","metadata":{},"outputs":[],"source":["Ok, now we can use the `GridSearchCV` to search the best parameters generating the highest F1 score.\n"]},{"cell_type":"code","id":"2a71ab4b-2c91-4f0b-93f0-32ab0b91f833","metadata":{},"outputs":[],"source":["grid_search = GridSearchCV(estimator = model, \n                        param_grid = params_grid, \n                        scoring='f1',\n                        cv = 5, verbose = 1)\ngrid_search.fit(X_train, y_train.values.ravel())\nbest_params = grid_search.best_params_"]},{"cell_type":"code","id":"b4384c77-548c-4695-9d32-21e7ab0a0011","metadata":{},"outputs":[],"source":["best_params"]},{"cell_type":"markdown","id":"df9c6de0-e924-4377-9a02-635ab18c70db","metadata":{},"outputs":[],"source":["So the best parameters are criterion=`gini`, max_depth=10, and min_samples_leaf=5.\n"]},{"cell_type":"markdown","id":"cffa190b-249d-4c31-afbd-bf11e88b2491","metadata":{},"outputs":[],"source":["## Coding exercise: build and visualize a decision tree with the best parameters\n"]},{"cell_type":"code","id":"f2bb4563-8726-4535-823b-3e5c9b7dae40","metadata":{},"outputs":[],"source":["## Type your code here\n"]},{"cell_type":"markdown","id":"49e85092-d050-4f90-8280-4bb93bd85629","metadata":{},"outputs":[],"source":["<details><summary>Click here for a sample solution</summary>\n","\n","```python\n","# Train a customized decision tree\n","custom_model = DecisionTreeClassifier(criterion='gini', max_depth=10, min_samples_leaf=5, random_state=rs)\n","custom_model.fit(X_train, y_train.values.ravel())\n","preds = custom_model.predict(X_test)\n","evaluate_metrics(y_test, preds)\n","# Plot the decision tree\n","plot_decision_tree(custom_model, feature_names)\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"0ecc6f03-ccaa-4fcf-a9c1-9729b1fc26f2","metadata":{},"outputs":[],"source":["## Next Steps\n"]},{"cell_type":"markdown","id":"2662ff74-2e8d-4345-906e-5c24cdca2481","metadata":{},"outputs":[],"source":["Now you have learned and applied the decision tree model to solve a real-world tumor type classification problem. You also visualized and fine-tuned the decision models. Later, you will continue learning other popular classification models with different structures, assumptions, cost functions, and application scenarios.\n"]},{"cell_type":"markdown","id":"63ba5326-7762-43eb-a606-ffd6948c9872","metadata":{},"outputs":[],"source":["## Authors\n"]},{"cell_type":"markdown","id":"3e74f19b-c777-4c89-b6cf-2d1aa4a7086b","metadata":{},"outputs":[],"source":["[Yan Luo](https://www.linkedin.com/in/yan-luo-96288783/)\n"]},{"cell_type":"markdown","id":"03d00c9d-a543-4e90-aa26-f9e6b3833868","metadata":{},"outputs":[],"source":["### Other Contributors\n"]},{"cell_type":"markdown","id":"f99531d2-f7e4-451c-a641-ca45e56457fb","metadata":{},"outputs":[],"source":["## Change Log\n"]},{"cell_type":"markdown","id":"c2aefcbe-fdee-4e3e-a2ac-d512a3a2014f","metadata":{},"outputs":[],"source":["|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n","|-|-|-|-|\n","|2021-9-23|1.0|Yan|Created the initial version||2022-2-9|1.1|Steve Hord|QA pass|\n"]},{"cell_type":"markdown","id":"d0f50ff6-f6e1-4fec-8a5e-23bd088a5afd","metadata":{},"outputs":[],"source":["Copyright © 2021 IBM Corporation. All rights reserved.\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"prev_pub_hash":"d052883c3dd9dbe4bd836f03dcb5a7a2b8f67ce8b932e7df85215455e721924d"},"nbformat":4,"nbformat_minor":4}